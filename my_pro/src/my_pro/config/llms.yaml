local_llama:
  name: "Local LLaMA 3.2 1B Instruct"
  type: "local"
  model_path: "C:/devhome/projects/models/meta-llama-Llama-3-2-1B-Instruct/meta-llama-Llama-3-2-1B-Instruct"
  max_tokens: 512
  temperature: 0.7
  top_p: 0.9
  repetition_penalty: 1.1
  device: "auto"  # Will automatically use CUDA if available, otherwise CPU
