# Local LLaMA Integration with Crew AI

This project demonstrates how to integrate a local LLaMA model with Crew AI, allowing your agents to use a locally hosted model instead of external API services.

## ğŸš€ Features

- **Local Model Integration**: Use your local LLaMA 3.2 1B Instruct model
- **Crew AI Compatible**: Full integration with Crew AI's agent system
- **Configurable**: Easy configuration through YAML files
- **GPU/CPU Support**: Automatic device detection and optimization
- **Error Handling**: Robust error handling and logging

## ğŸ“ Project Structure

```
my_pro/
â”œâ”€â”€ src/my_pro/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ agents.yaml          # Agent configurations with LLM settings
â”‚   â”‚   â”œâ”€â”€ tasks.yaml           # Task definitions
â”‚   â”‚   â”œâ”€â”€ llms.yaml            # LLM configuration
â”‚   â”‚   â””â”€â”€ __init__.py          # Configuration loader
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ local_llm.py         # Local LLaMA LLM integration
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ crew.py                  # Updated crew with local LLM
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ test_local_llm.py            # Test script
â””â”€â”€ README_LOCAL_LLM.md          # This file
```

## ğŸ› ï¸ Installation

1. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Download LLaMA Model**:
   - Download the LLaMA 3.2 1B Instruct model to your local system
   - Update the model path in `src/my_pro/config/llms.yaml`

3. **Verify Model Path**:
   Ensure your model path exists and contains the necessary model files:
   ```
   C:/devhome/projects/models/meta-llama-Llama-3-2-1B-Instruct/
   â”œâ”€â”€ config.json
   â”œâ”€â”€ pytorch_model.bin
   â”œâ”€â”€ tokenizer.model
   â””â”€â”€ tokenizer_config.json
   ```

## âš™ï¸ Configuration

### LLM Configuration (`llms.yaml`)

```yaml
local_llama:
  name: "Local LLaMA 3.2 1B Instruct"
  type: "local"
  model_path: "C:/devhome/projects/models/meta-llama-Llama-3-2-1B-Instruct/meta-llama-Llama-3-2-1B-Instruct"
  max_tokens: 512
  temperature: 0.7
  top_p: 0.9
  repetition_penalty: 1.1
  device: "auto"  # Will automatically use CUDA if available, otherwise CPU
```

### Agent Configuration (`agents.yaml`)

```yaml
researcher:
  role: "{topic} Senior Data Researcher"
  goal: "Uncover cutting-edge developments in {topic}"
  backstory: "You're a seasoned researcher..."
  llm: local_llama          # Specify which LLM to use
  verbose: true
  allow_delegation: false
```

## ğŸ”§ Usage

### 1. Basic Integration

The local LLM is automatically integrated into your crew:

```python
from my_pro.crew import MyPro

# Create crew instance
crew = MyPro()

# Run your crew - agents will automatically use the local LLM
result = crew.kickoff()
```

### 2. Testing the Integration

Run the test script to verify everything works:

```bash
python test_local_llm.py
```

### 3. Custom Model Path

You can override the model path when creating the LLM:

```python
from my_pro.tools.local_llm import LocalLLaMALLM

# Custom model path
custom_llm = LocalLLaMALLM(
    model_path="/path/to/your/model"
)
```

## ğŸ“Š Model Information

Get detailed information about your loaded model:

```python
from my_pro.tools.local_llm import local_llm

# Check if model is loaded
print(local_llm.is_model_loaded())

# Get model information
info = local_llm.get_model_info()
print(info)
```

## ğŸš¨ Troubleshooting

### Common Issues

1. **Model Path Not Found**:
   - Verify the model path in `llms.yaml`
   - Ensure the directory contains all required model files

2. **CUDA Out of Memory**:
   - Reduce `max_tokens` in the configuration
   - Use CPU mode by setting `device: "cpu"`

3. **Import Errors**:
   - Ensure all dependencies are installed: `pip install -r requirements.txt`
   - Check Python path and module structure

4. **Model Loading Failures**:
   - Verify model files are not corrupted
   - Check available disk space and memory

### Debug Mode

Enable detailed logging by modifying the logging level in `local_llm.py`:

```python
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ”„ Updating Configuration

### Adding New LLMs

1. Add configuration to `llms.yaml`:
   ```yaml
   new_model:
     name: "New Model Name"
     type: "local"
     model_path: "/path/to/model"
     # ... other parameters
   ```

2. Update agent configurations to use the new LLM:
   ```yaml
   agent_name:
     # ... other config
     llm: new_model
   ```

### Modifying Model Parameters

Update the parameters in `llms.yaml` and restart your application:

```yaml
local_llama:
  max_tokens: 1024        # Increase token limit
  temperature: 0.5        # Reduce randomness
  top_p: 0.95            # Increase diversity
```

## ğŸ“ˆ Performance Tips

1. **GPU Usage**: Ensure CUDA is available for optimal performance
2. **Memory Management**: Monitor memory usage and adjust `max_tokens` accordingly
3. **Batch Processing**: Consider processing multiple requests in batches
4. **Model Optimization**: Use model quantization for reduced memory usage

## ğŸ¤ Contributing

To extend this integration:

1. **Add New Model Types**: Extend the `LocalLLaMALLM` class
2. **Improve Error Handling**: Add more specific error types and recovery
3. **Add Metrics**: Implement performance monitoring and logging
4. **Optimize Generation**: Improve response quality and speed

## ğŸ“š Additional Resources

- [Crew AI Documentation](https://docs.crewai.com/)
- [Transformers Documentation](https://huggingface.co/docs/transformers/)
- [PyTorch Documentation](https://pytorch.org/docs/)
- [LLaMA Model Information](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)

## ğŸ“„ License

This project follows the same license as your main project. Ensure compliance with LLaMA model licensing requirements.
